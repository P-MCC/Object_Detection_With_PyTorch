{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (8.3.34)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.35-py3-none-any.whl (887 kB)\n",
      "     -------------------------------------- 887.4/887.4 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.23.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (1.26.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (2.5.1+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (0.20.1+cu118)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (4.67.0)\n",
      "Requirement already satisfied: psutil in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (6.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from ultralytics) (2.0.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n",
      "Requirement already satisfied: networkx in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in m:\\codes-scripts\\python_projects\\object_detection_with_pytorch\\env\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Installing collected packages: ultralytics\n",
      "  Attempting uninstall: ultralytics\n",
      "    Found existing installation: ultralytics 8.3.34\n",
      "    Uninstalling ultralytics-8.3.34:\n",
      "      Successfully uninstalled ultralytics-8.3.34\n",
      "Successfully installed ultralytics-8.3.35\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\env\\Lib\\site-packages\\torch\\hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to ./Weights\\master.zip\n",
      "YOLOv5  2024-11-22 Python-3.11.1 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:07<00:00, 1.89MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=./yolov5s.pt' with new 'model=./yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "New https://pypi.org/project/ultralytics/8.3.36 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.35  Python-3.11.1 torch-2.5.1+cu118 CPU (AMD Ryzen 5 4600H with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=./yolov5s.pt, data=M:/Datasets/WoodPile_YOLOv5/data.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=m:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\runs\\detect\\train4\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
      " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
      " 24        [17, 20, 23]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "YOLOv5s summary: 262 layers, 9,122,579 parameters, 9,122,563 gradients, 24.0 GFLOPs\n",
      "\n",
      "Transferred 421/427 items from pretrained weights\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning M:\\Datasets\\WoodPile_YOLOv5\\train\\labels.cache... 1361 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1361/1361 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  M:\\Datasets\\WoodPile_YOLOv5\\train\\images\\images-22-_jpeg_jpg.rf.4977728e58867bde27a6b26acfb44576.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  M:\\Datasets\\WoodPile_YOLOv5\\train\\images\\images-22-_jpeg_jpg.rf.6c611963d3fb76a5be6e683f7d7c73a1.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  M:\\Datasets\\WoodPile_YOLOv5\\train\\images\\images-22-_jpeg_jpg.rf.8be896cd5f301a6b2c7ee80a74841cc9.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning M:\\Datasets\\WoodPile_YOLOv5\\valid\\labels.cache... 130 images, 0 backgrounds, 0 corrupt: 100%|██████████| 130/130 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to m:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\runs\\detect\\train4\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mm:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\runs\\detect\\train4\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3         0G      1.407      1.245      1.172         38        640: 100%|██████████| 86/86 [16:25<00:00, 11.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:42<00:00,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        130       9258      0.697      0.603      0.642      0.404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3         0G      1.201     0.8393      1.047        200        640: 100%|██████████| 86/86 [16:30<00:00, 11.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:43<00:00,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        130       9258      0.795      0.764      0.811       0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3         0G      1.099     0.7509      1.012         37        640: 100%|██████████| 86/86 [16:14<00:00, 11.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:44<00:00,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        130       9258      0.832      0.821      0.864      0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epochs completed in 0.858 hours.\n",
      "Optimizer stripped from m:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\runs\\detect\\train4\\weights\\last.pt, 18.5MB\n",
      "Optimizer stripped from m:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\runs\\detect\\train4\\weights\\best.pt, 18.5MB\n",
      "\n",
      "Validating m:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\runs\\detect\\train4\\weights\\best.pt...\n",
      "Ultralytics 8.3.35  Python-3.11.1 torch-2.5.1+cu118 CPU (AMD Ryzen 5 4600H with Radeon Graphics)\n",
      "YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:25<00:00,  5.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        130       9258      0.831      0.821      0.864      0.611\n",
      "Speed: 1.8ms preprocess, 151.6ms inference, 0.0ms loss, 5.4ms postprocess per image\n",
      "Results saved to \u001b[1mm:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\runs\\detect\\train4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"./yolov5s.pt\")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=\"M:/Datasets/WoodPile_YOLOv5/data.yaml\",  # path to dataset YAML\n",
    "    epochs=3,  # number of training epochs\n",
    "    imgsz=640,  # training image size\n",
    "    device=\"cpu\",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.35  Python-3.11.1 torch-2.5.1+cu118 CPU (AMD Ryzen 5 4600H with Radeon Graphics)\n",
      "YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning M:\\Datasets\\WoodPile_YOLOv5\\valid\\labels.cache... 130 images, 0 backgrounds, 0 corrupt: 100%|██████████| 130/130 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:23<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        130       9258      0.831      0.821      0.864      0.611\n",
      "Speed: 1.6ms preprocess, 138.9ms inference, 0.0ms loss, 5.0ms postprocess per image\n",
      "Results saved to \u001b[1mm:\\Codes-Scripts\\Python_Projects\\Object_Detection_With_PyTorch\\runs\\detect\\train42\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on the validation set\n",
    "metrics = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 M:\\Datasets\\WoodPile_YOLOv5\\train\\images\\00l0l_dCzNHgxQFkf_0uH0t2_600x450_jpg.rf.cf6ccc93dc655fd62a1b18b7fc5d7966.jpg: 640x640 21 woods, 142.0ms\n",
      "Speed: 3.0ms preprocess, 142.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Number of wood instances detected: 21\n"
     ]
    }
   ],
   "source": [
    "results = model(\"M:/Datasets/WoodPile_YOLOv5/train/images/00l0l_dCzNHgxQFkf_0uH0t2_600x450_jpg.rf.cf6ccc93dc655fd62a1b18b7fc5d7966.jpg\")\n",
    "results[0].show()\n",
    "\n",
    "wood_count = len(results[0].boxes)  # Each bounding box represents one detection\n",
    "print(f\"Number of wood instances detected: {wood_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
